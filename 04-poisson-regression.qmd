# Poisson Regression

Stub for Poisson regression content

```{r packages}
library("tidyverse"); theme_set(theme_bw())
library("ggResidpanel")
```

## Model

Let $Y_i\in\{0,1,2,\ldots\}$ be a count (typically over some amount of time
or some amount of space) with associated explanatory variables 
$X_{i,1}, \ldots, X_{i,p}$.

Then a Poisson regression model is 
$$
Y_i \stackrel{ind}{\sim} Po(\lambda_i) 
$$
and
$$
\log(\lambda_i) 
= \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \cdots +\beta_p X_{i,p} 
$$

As a reminder, $E[Y_i] = Var[Y_i] = \lambda_i$ and thus the variance of the 
observations increases as the mean increases. 


## Interpretation

When all explanatory variables are zero, then 
$$ 
E[Y_i|X_{i,1} = 0, \ldots,X_{i,p} = 0]  = \lambda_i  = e^{\beta_0}
$$

thus $\beta_0$ determines the **expected response when all explanatory variables are zero**.

More generally, 
$$ 
E[Y_i|X_{i,1}=x_1, \ldots,X_{i,p}=x_p] 
= e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}.
$$

If $X_{i,1}$  increases by one unit, we have 
$$ 
E[Y_i|X_{i,1}=x_1+1, \ldots,X_{i,p}=x_p] 
= e^{\beta_0+\beta_1(x_1+1)+\cdots+\beta_px_p}
= e^{\beta_0+\beta_1x_1+\cdots+\beta_px_p}e^{\beta_1}
$$

Thus 
$$
\frac{E[Y_i|X_{i,1}=x_1+1, \ldots,X_{i,p}=x_p]}{E[Y_i|X_{i,1}=x_1\phantom{+1}\,, \ldots,X_{i,p}=x_p]} 
= e^{\beta_1}.
$$

Thus $e^{\beta_p}$ is the **multiplicative effect on the mean response for a one unit increase in the associated explanatory variable when holding all other explanatory variables constant**.

Rather than reporting the multiplicate effect, 
it is common to report the **percentage increase** (or decrease).
To obtain the percentage increase, we calculate
$$100(e^{\beta_p} - 1).$$

## Assumptions

- Observations are independent 
- Observations have a Poisson distribution 
  - Count
  - Variance is equal to the mean
- Relationship between expected response relationship and the explanatory variables is given by the model

## Diagnostics

Diagnostics in Poisson regression models is much harder because we don't have 
the same idea of residuals as we had for linear regression models. 
We can compute some residuals:
$$
r_i = Y_i - \hat{Y}_i = Y_i - \lambda_i
$$
but these residuals each have a different variance. 
Thus, we need to standardize by calculating the *Pearson residual*
$$
r_i^P =  \frac{Y_i - \lambda_i}{\sqrt{\lambda_i}}
$$


## Superbowl scores

```{r superbowl-scores-data}
get_points <- function(points_with_text) {
  as.numeric(str_extract(points_with_text, "(\\d)+"))
}

# From https://www.espn.com/nfl/superbowl/history/winners
superbowl_scores <- read_csv('data/superbowl_scores.csv') |>
  tidyr::separate(RESULT, 
                  c("winning_points",
                    "losing_points"),
                  sep = ", ") |>
  
  # Extract points 
  mutate(
    winning_points = get_points(winning_points),
    losing_points  = get_points(losing_points)
  ) |>
  
  # Create single `points` variable
  pivot_longer(cols = c("winning_points", "losing_points"),
               names_to = "team",
               values_to = "points") |>
  mutate(
    team = gsub("_points", "", team), 
    
    # Extract year
    DATE = as.Date(DATE, format = "%b. %d, %Y"),
    year = as.numeric(format(DATE, format = "%Y"))
  )

head(superbowl_scores)
summary(superbowl_scores)
```

### Points over time

Scientific question: How has the number of points changed over time?

```{r superbowl-winning-points-plot}
ggplot(superbowl_scores,
       aes(x = year, y = points)) + 
  geom_point() +
  scale_y_log10() + # Consistent with Poisson regression
  labs(
    x = "Year",
    y = 'Points scored by the winning team',
    title = 'Superbowl'
  )
```

Let's fit a Poisson regression model using points as the response 
variable and year as the explanatory variable. 

```{r superbowl-winning-points-poisson-regression}
# Poisson regression
m <- glm(points ~ year,           # use glm()
         data = superbowl_scores, 
         family = "poisson")      # Poisson regression

m
```

```{r diagnostics}
ggResidpanel::resid_panel(m,
                          qqbands = TRUE,
                          smoother = TRUE,
                          type = "pearson")
```


```{r superbowl-winning-point-poisson-regression-summary}
summary(m)
```

```{r poisson-regression-interpretation}
# Estimates and confidence intervals
cbind(coef(m), confint(m)) |> exp() |> round(3)

# Calculate the percentage increase
perinc <- 100*(exp(c(coef(m)[2], confint(m)[2,])) - 1) 
perinc |> round(2)
```

::: {.callout-note}

## Interpretation

We ran a Poisson regression using number of points each team scored as the 
response variable and year as the explanatory variable.
According to the model, the expected points in year 0 for the winning team is 
`r round(exp(coef(m)[1]))` with a 95\% confidence
interval of (`r round(exp(confint(m)[1,1]))`, `r round(exp(confint(m)[1,2]))`).
The percentage increase in points by the winning team per year is
`r round(perinc[1], 2)`\% (`r round(perinc[2], 2)`\%, `r round(perinc[3], 2)`\%).
:::


The Poisson regression does indicate some *overdispersion*, 
i.e. that the variance is larger than the mean. 
A quick check looks at the residual deviance relative to its degrees of freedom.
If there is no overdispersion, then the residual deviance should have a 
chi-squared distribution with the indicated degrees of freedom. 
Thus, if the residual deviance is very large compared to its degrees of freedom
we likely have overdispersion. 

```{r overdispersion-test}
# p-value for an overdispersion test
1 - pchisq(m$deviance, m$df.residual)
```


```{r superbowl-scores-plot-with-line}
ggplot(superbowl_scores,
       aes(x = year,
           y = points)) +
  geom_point() +
  scale_y_log10() +
  geom_smooth(method = "glm")
```

### Winning points vs losing points

Simple regression using binary variable 

```{r superbowl-points-winning-v-losing}
m <- glm(points ~ team, 
         data = superbowl_scores,
         family = "poisson")

summary(m)
```

```{r superbowl-points-winning-v-losing-interpretation}
cbind(coef(m), confint(m)) |> exp() |> round(3)

perinc <- 100*(exp(c(coef(m)[2], confint(m)[2,])) - 1) 
perinc
```


::: {.callout-note}

## Interpretation

We ran a Poisson regression with number of points scored and winning/losing
team as the explanatory variable.
According to the model, the expected points for the losing team is 
`r round(exp(coef(m)[1]))` with a 95\% confidence
interval of (`r round(exp(confint(m)[1,1]))`, `r round(exp(confint(m)[1,2]))`).
The percentage increase in points by the winning team compared to the losing 
team is
`r round(perinc[1])`\% (`r round(perinc[2])`\%, `r round(perinc[3])`\%).
:::


### Points after adjusting for year


```{r superbowl-points-winning-v-losing-year}
m <- glm(points ~ team + year, 
         data = superbowl_scores,
         family = "poisson")

summary(m)
```

```{r superbowl-points-winning-v-losing-year-interpretation}
cbind(coef(m), confint(m)) |> exp() |> round(3)

perinc <- 100*(exp(cbind(coef(m)[2:3], confint(m)[2:3,])) - 1) 
perinc
```

::: {.callout-note}

## Interpretation

We ran a Poisson regression with number of points scored as the response 
variable and winning/losing team and year as the explanatory variables.
According to the model, the expected points for the losing team in year 0 is 
`r round(exp(coef(m)[1]), 1)` with a 95\% confidence
interval of (`r round(exp(confint(m)[1,1]), 1)`, `r round(exp(confint(m)[1,2]), 1)`).
After adjusting for year, the percentage increase in expected points by the winning team compared to the losing team is
`r round(perinc[1,1], 1)`\% (`r round(perinc[1,2], 1)`\%, `r round(perinc[1,3], 1)`\%).
After adjusting for winning versus losing team, 
the percentage increase in expected points by the both teams per year is
`r round(perinc[2,1], 1)`\% (`r round(perinc[2,2], 1)`\%, `r round(perinc[2,3], 1)`\%).
:::

### Interaction

```{r superbowl-points-winning-v-losing-year-interaction}
m <- glm(points ~ team * year,     # Note the * rather than +
         data = superbowl_scores,
         family = "poisson")

summary(m)
```

```{r superbowl-points-winning-v-losing-year-interaction-interpretation}
cbind(coef(m), confint(m)) |> exp()

perinc <- 100*(exp(cbind(coef(m)[2:3], confint(m)[2:3,])) - 1) 
perinc
```



```{r superbowl-interaction-plot}
ggplot(superbowl_scores,
       aes(x = year,
           y = points,
           color = team,
           shape = team)) + 
  geom_point() + 
  scale_y_log10() +
  geom_smooth(method = "glm")
```
