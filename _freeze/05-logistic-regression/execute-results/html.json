{
  "hash": "75beb4450526cf0b569240e8ebecf77d",
  "result": {
    "engine": "knitr",
    "markdown": "# Logistic Regression\n\nStub for logistic regression content\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\"); theme_set(theme_bw())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: package 'purrr' was built under R version 4.4.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4.9000     ✔ readr     2.1.5     \n✔ forcats   1.0.0          ✔ stringr   1.5.1     \n✔ ggplot2   3.5.1          ✔ tibble    3.2.1     \n✔ lubridate 1.9.3          ✔ tidyr     1.3.1     \n✔ purrr     1.0.4          \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"ggResidpanel\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20250212) # Reproducible simulations\n```\n:::\n\n\n\n## Model\n\nLet $Y_i$ be the count out of $n_i$ attempts where each attempt is independent\nand there is a common probability of success\n$\\pi_i$. \nWe assume \n$$Y_i \\stackrel{ind}{\\sim} Bin(n_i, \\pi_i).$$\nA special case is when \n$n_i = 1$ and $Y_i$ is a binary variable. \n\nFor each $i$, we have a collection of explanatory variables\n$X_{i,1}, \\ldots X_{i,p}$.\n\nThen a logistic regression model assumes\n$$\n\\mbox{logit}(\\pi_i)=\\mbox{log}\\left( \\frac{\\pi_i}{1-\\pi_i} \\right) =\n\\eta = \n\\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots +\\beta_p X_{i,p} \n$$\n\nwhere the logistic function of $X_i$ is \n$$ \n\\pi_i = f(X_i) \n= \\frac{e^{\\beta_0+\\beta_1 X_{i}}}{1+e^{\\beta_0+\\beta_1 X_{i}}} \n=  \\frac{1}{1+e^{-(\\beta_0+\\beta_1 X_{i})}}.\n$$\n\n### Interpretation\n\nWhen $X_{i,j} = 0$ for all $j$, then \n$$ \nE[Y_i|X_{i,1} = 0, \\ldots,X_{i,p} = 0] = \\pi_i = \\frac{1}{1+e^{-\\beta_0}}\n$$\n\nthus $\\beta_0$ determines the **probability of success when the \nexplanatory variable is zero**.\n\nThe odds of success when $X_{i,j} = x_j$ is \n$$ \n\\frac{\\pi_1}{1-\\pi_1} = e^{\\beta_0+\\beta_1x_1 + \\cdots \\beta_p x_p}.\n$$\n\nThe probability of success when $X_1 =x_1 + 1$ and the remainder are left at\n$X_{i,j} =x_j$\nis \n$$ \n\\frac{\\pi_2}{1-\\pi_2} = e^{\\beta_0+\\beta_1(x_1+1) + \\cdots \\beta_p x_p}\n= e^{\\beta_0+\\beta_1x_1 + \\cdots \\beta_p x_p}e^{\\beta_1} =\n\\frac{\\pi_1}{1-\\pi_1} e^{\\beta_1}.\n$$\n\nThus, the multiplicative change in the odds for a 1 unit increase in $X_1$ is \n$$\n\\frac{\\frac{\\pi_2}{1-\\pi_2}}{\\frac{\\pi_1}{1-\\pi_1}} \n= e^{\\beta_1}\n$$\n\nThis is also referred to as an \\alert{odds ratio}.\n\n### Diagnostics\n\n\n\n\n\n## WNBA Shooting\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwnba_shots <- read_csv(\"data/wnba-shots-2021.csv\") |>\n  filter( !(grepl(\"Free Throw\", shot_type)) ) |>\n  mutate(\n    distance = sqrt((coordinate_x - 25)^2 + (coordinate_y - 0)^2)\n  ) |>\n  filter( !is.na(distance), !is.na(made_shot))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 41497 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): desc, shot_type, shooting_team, home_team_name, away_team_name\ndbl (10): game_id, game_play_number, shot_value, coordinate_x, coordinate_y,...\nlgl  (1): made_shot\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check for incorrectly coded data\nwnba_shots |> filter(shot_value == 2 & distance > 22 + 1.75 / 12) |> select(distance, coordinate_x, coordinate_y) |> arrange(distance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 17 × 3\n   distance coordinate_x coordinate_y\n      <dbl>        <dbl>        <dbl>\n 1     22.2           28           22\n 2     22.2           22           22\n 3     22.2           38           18\n 4     22.2           22           22\n 5     22.2           38           18\n 6     22.2           22           22\n 7     22.4           15           20\n 8     22.4           35           20\n 9     22.4           15           20\n10     22.5           13           19\n11     22.5           37           19\n12     22.5           17           21\n13     22.5           37           19\n14     22.8           31           22\n15     22.8           16           21\n16     23             25           23\n17     23.0           12           19\n```\n\n\n:::\n\n```{.r .cell-code}\nwnba_shots |> filter(shot_value == 3 & distance < 22 + 1.75 / 12) |> select(distance, coordinate_x, coordinate_y) |> arrange(distance)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 86 × 3\n   distance coordinate_x coordinate_y\n      <dbl>        <dbl>        <dbl>\n 1     21.6            4            5\n 2     21.8            4            6\n 3     21.8           46            6\n 4     22              3            0\n 5     22             47            0\n 6     22              3            0\n 7     22              3            0\n 8     22              3            0\n 9     22              3            0\n10     22              3            0\n# ℹ 76 more rows\n```\n\n\n:::\n:::\n\n\n### Sylvia Fowles probability vs distance\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract Sylvia Fowles shots\nfowles_shots <- wnba_shots |>\n  filter(str_starts(desc, \"Sylvia Fowles\"))\n\ntable(fowles_shots$made_shot)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nFALSE  TRUE \n  122   187 \n```\n\n\n:::\n\n```{.r .cell-code}\naggregate_fowles_shots <- fowles_shots |>\n  mutate(\n    distanceR = round(distance)\n  ) |>\n  group_by(distanceR) |>\n  summarize(\n    n = n(),\n    y = sum(made_shot), # TRUE gets converted to 1 and FALSE to 0\n    p = y/n\n  )\n\ng <- ggplot(aggregate_fowles_shots,\n       aes(x = distanceR, y = p, size = n)) +\n  geom_point()\n\ng\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/fowles-data-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- glm(made_shot ~ distance, # made_shot \n         data = fowles_shots,\n         family = \"binomial\")  \n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresid_panel(m)\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/fowles-probability-by-distance-diagnostics-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = made_shot ~ distance, family = \"binomial\", data = fowles_shots)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.92302    0.17341   5.323 1.02e-07 ***\ndistance    -0.11297    0.02924  -3.863 0.000112 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 414.59  on 308  degrees of freedom\nResidual deviance: 397.80  on 307  degrees of freedom\nAIC: 401.8\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nexpit <- function(eta) {\n  1 / (1 + exp(-eta))\n}\n\ncbind(coef(m)[1], confint(m)[1,]) |> expit()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]      [,2]\n2.5 %  0.7156569 0.6430326\n97.5 % 0.7156569 0.7806552\n```\n\n\n:::\n\n```{.r .cell-code}\ncbind(coef(m)[2], confint(m)[2,]) |> exp()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]      [,2]\n2.5 %  0.8931809 0.8410906\n97.5 % 0.8931809 0.9439774\n```\n\n\n:::\n:::\n\n::: {.callout-note}\n\n## Interpretation\n\nWe ran a logistic regression using made shot as a success and distance from the\nhoop as the explanatory variable.\nAccording to the model, the probability of making a shot at a distance of 0 is  \n0.72 with a 95\\% confidence\ninterval of (0.64, 0.78).\nFor each additional foot away from the hoop the log odds of making the shot is \nmultiplied by \n0.89 with a 95\\% confidence\ninterval of (0.84, 0.94).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create data frame with fitted values\nfowles_fitted <- fowles_shots |>\n  mutate(p = predict(m, type = \"response\"))\n\nggplot(fowles_fitted,\n       aes(x = distance)) +\n  geom_point(aes(y = as.numeric(made_shot))) +\n  geom_line(aes(y = p), col = 'blue') +\n  ylim(0, 1)\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/fowles-probability-by-distance-fitted-plot-1.png){width=672}\n:::\n:::\n\n\n\n### Sylvia Fowles vs Sue Bird\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntwo_player_shots <- wnba_shots |>\n  filter(str_starts(desc, \"Sylvia Fowles\") | \n           str_starts(desc, \"Sue Bird\") ) |>\n  mutate(player = ifelse(str_starts(desc, \"Sylvia Fowles\"), \n                         \"Sylvia Fowles\", \n                         \"Sue Bird\"))\n\ntwo_player_summary <- two_player_shots |>\n  group_by(player) |>\n  summarize(\n    n = n(),\n    y = sum(made_shot),\n    p = y / n\n  )\n\ntwo_player_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  player            n     y     p\n  <chr>         <int> <int> <dbl>\n1 Sue Bird        267   107 0.401\n2 Sylvia Fowles   309   187 0.605\n```\n\n\n:::\n:::\n\n\nHere we see that Sylvia Fowles is the better field goal shooter by field \ngoal percent. \nThis is consistent (although the numbers are somewhat different)\nwith the [WNBA 2021 Shooting Statistics](https://www.foxsports.com/wnba/stats?category=shooting&sort=fgpct&season=2021&seasonType=reg&sortOrder=desc).\n\nWe could fit a comparison of two binomial proportions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npt <- prop.test(two_player_summary$y, two_player_summary$n)\npt\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\t2-sample test for equality of proportions with continuity correction\n\ndata:  two_player_summary$y out of two_player_summary$n\nX-squared = 23.143, df = 1, p-value = 1.504e-06\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.2880795 -0.1207783\nsample estimates:\n   prop 1    prop 2 \n0.4007491 0.6051780 \n```\n\n\n:::\n:::\n\nAn appealing aspect of this approach is that it directly provides an estimate\nwith uncertainty about the difference between the two probabilities. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff(pt$estimate)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   prop 2 \n0.2044289 \n```\n\n\n:::\n\n```{.r .cell-code}\npt$conf.int\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2880795 -0.1207783\nattr(,\"conf.level\")\n[1] 0.95\n```\n\n\n:::\n:::\n\n\n\nAlternatively we can fit a logistic regression model. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmP <- glm(made_shot ~ player,\n         data = two_player_shots,\n         family = \"binomial\")\n\nsummary(mP)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = made_shot ~ player, family = \"binomial\", data = two_player_shots)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -0.4023     0.1249  -3.222  0.00127 ** \nplayerSylvia Fowles   0.8294     0.1707   4.859 1.18e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 798.26  on 575  degrees of freedom\nResidual deviance: 774.14  on 574  degrees of freedom\nAIC: 778.14\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nWe have do perform some transformations to get interpretable parameters. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(mP)[1], confint(mP)[1,]) |> expit() # Probability for reference level\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]      [,2]\n2.5 %  0.4007491 0.3431224\n97.5 % 0.4007491 0.4602700\n```\n\n\n:::\n\n```{.r .cell-code}\ncbind(coef(mP)[2], confint(mP)[2,]) |> exp()   # Multicative effect on log odds\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWaiting for profiling to be done...\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]     [,2]\n2.5 %  2.292018 1.643074\n97.5 % 2.292018 3.209523\n```\n\n\n:::\n:::\n\n\n::: {.callout-note}\n\n## Interpretation\n\nWe ran a logistic regression using the number of made shots out of the total\nand used player (Sylvia Fowles vs Sue Bird) as the explanatory variable.\nAccording to the model, the probability of making a shot for Sue Bird is  \n0.4 with a 95\\% confidence\ninterval of (0.34, 0.46) and\nthe multiplicative effect on log odds for Sylvia Fowles compared to Sue Bird is\n2.29 with a 95\\% confidence\ninterval of (1.64, 3.21).\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmPD <- glm(made_shot ~ player + distance,\n         data = two_player_shots,\n         family = \"binomial\")\n\nsummary(mPD)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = made_shot ~ player + distance, family = \"binomial\", \n    data = two_player_shots)\n\nCoefficients:\n                    Estimate Std. Error z value Pr(>|z|)   \n(Intercept)          0.51006    0.31922   1.598   0.1101   \nplayerSylvia Fowles  0.11039    0.28761   0.384   0.7011   \ndistance            -0.04398    0.01416  -3.105   0.0019 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 798.26  on 575  degrees of freedom\nResidual deviance: 764.33  on 573  degrees of freedom\nAIC: 770.33\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\nIn the first model, it appears that Sylvia Fowles is the better shooter. \nIn this model, it appears that there is no significant difference between\nSylvia Fowles and Sue Bird. \nThis is an example of Simpson's Paradox,\ni.e. our understanding of a relationship changes as we adjust for other \nexplanatory variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmPDI <- glm(made_shot ~ player * distance,\n         data = two_player_shots,\n         family = \"binomial\")\n\nsummary(mPDI)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = made_shot ~ player * distance, family = \"binomial\", \n    data = two_player_shots)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                  -0.004762   0.363577  -0.013  0.98955   \nplayerSylvia Fowles           0.927781   0.402815   2.303  0.02127 * \ndistance                     -0.019103   0.016459  -1.161  0.24577   \nplayerSylvia Fowles:distance -0.093863   0.033558  -2.797  0.00516 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 798.26  on 575  degrees of freedom\nResidual deviance: 756.00  on 572  degrees of freedom\nAIC: 764\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n## Golf putting\n\n@fearing2011catch fit a logistic regression model for the probability of \nmaking a putt as a function of the distance of the putt. \nThe resulting function they used was\n\n$$\n\\mbox{logit}(\\pi) = \n\\beta_0 + \\beta_1 d + \\beta_2 d^2 + \\beta_3 d^3 + \\beta_4 d^4 + \\beta_5 \\log(d)\n$$\n\nwith estimated values of \n\n$$\n\\mbox{logit}(\\pi) = \n7.31e0 -5.58e0 d + 6.76e-1d^2 -1.97e-2d^3 + 2.93e-4 d^4 -1.62e-6\\log(d)\n$$\n\nI'm explicitly using scientific notation here to try and assure that I don't \nmake a typo. \n\nEncoded in an R function, we have\n\n\n::: {.cell}\n\n```{.r .cell-code}\nputt_probability <- function(d) {\n  eta <- 7.31 - 5.58e0 * d + \n    6.76e-1 * d^2 -1.97e-2 * d^3 + \n    2.93e-4 * d^4 -1.62e-6 * log(d)\n  \n  return( 1 / (1 + exp(-eta) ) )\n}\n```\n:::\n\n\nWe can plot this function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data.frame(d = 2:100),\n       aes(x = d)) +\n  stat_function(fun = putt_probability)\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/probability-plot-1.png){width=672}\n:::\n:::\n\n\nHuh?!?!?! This looks nothing like the authors estimated probability in \nFigure 3. I tried a variety of variations, e.g. using $log$ base 10 and\nforgetting the negative sign in the exponential, but nothing seems to reproduce\nthe plot these authors have. \n\nSince we don't have their actual plot, \nI'll try to recreate their data from Figure 2. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulated_putts <- data.frame(\n  n = 1000, \n  \n  # I measured the height in the pdf\n  p = c(12.9, 12.5, 11.1, 7.5, 5.8, 4.5, 3.7, 3, 2.5, 2.1, 1.8, 1.5,\n        1.4, 1.2, 1.1, 1.0, .9, .8, .7, .6, .5, .5, .5, .5, \n        rep(0.4, 6), rep(0.3, 10), rep(0.2, 16), rep(0.1, 26),\n        rep(0.05, 18)) /\n    13 # total height\n) |>\n  mutate(\n    d = seq(2, by = 2, length = n()),\n    y = rbinom(n(), size = n, prob = p)\n  )\n```\n:::\n\n\nLet's plot the data to recreate Figure 2 from the paper.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simulated_putts, \n       aes(x = d, y = y / n)) +\n  geom_point(color = 'red') + \n  geom_line(color = 'red')\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/golf-data-plot-1.png){width=672}\n:::\n:::\n\n\n\nLet's fit a logistic regression model to these data.\nWe'll start with just using distance, \n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_d <- glm(cbind(y, n - y) ~ d, # cbind(successes, failures)\n           data = simulated_putts,\n           family = \"binomial\")        # logistic regression \n```\n:::\n\n\nDiagnostic plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggResidpanel::resid_panel(m_d, \n                          qqbands = TRUE,\n                          smoother = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/golf-distance-resid-panel-1.png){width=672}\n:::\n:::\n\n\nThis plot shows clear violation of model assumptions. \nIf we take a look at residuals vs distance plot we can see why. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nggResidpanel::resid_xpanel(m_d, \n                          smoother = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/golf-distance-resid-xpanel-1.png){width=672}\n:::\n:::\n\n\nWe could also plot the estimated \n\n\n\nThus, it is clear we need an improved model, \ni.e. we need to include a higher order term for distance. \nLet's try the terms the original authors used. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_d2 <- glm(cbind(y, n - y) ~ d + I(d^2), \n            data = simulated_putts,\n            family = \"binomial\")        \n```\n:::\n\n\nIn a generalized linear model, e.g. logistic regression,\nwe can use a drop-in-deviance test to evaluate the strength of evidence for \nterms in a model. \nIf we look at sequential tests, the default in `anova()` then \nwe can just keep terms \n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(y, n - y)\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                      99    24784.1              \nd       1  18984.9        98     5799.1 < 2.2e-16 ***\nI(d^2)  1   2934.3        97     2864.9 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nLike the authors, all of these terms are significant. \nWe could have even tried more terms, \nbut let's stick with these. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m_d2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(y, n - y) ~ d + I(d^2), family = \"binomial\", \n    data = simulated_putts)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.081e+00  2.849e-02   37.93   <2e-16 ***\nd           -1.099e-01  1.301e-03  -84.48   <2e-16 ***\nI(d^2)       4.276e-04  6.972e-06   61.34   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 24784.1  on 99  degrees of freedom\nResidual deviance:  2864.9  on 97  degrees of freedom\nAIC: 3329.9\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\nWhile the `anova()` output shows sequential tests, \ni.e. adding one term at a time, this analysis suggests that,\nafter including the logarithm of distance, \nwe perhaps didn't need any of the other distances. \nSo let's rearrange the terms.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_d3 <- glm(cbind(y, n - y) ~ log(d) + d + I(d^2) + I(d^3) + I(d^4), \n            data = simulated_putts,\n            family = \"binomial\")\n\nanova(m_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(y, n - y)\n\nTerms added sequentially (first to last)\n\n       Df Deviance Resid. Df Resid. Dev  Pr(>Chi)    \nNULL                      99    24784.1              \nlog(d)  1  24304.5        98      479.5 < 2.2e-16 ***\nd       1    205.8        97      273.7 < 2.2e-16 ***\nI(d^2)  1    107.9        96      165.8 < 2.2e-16 ***\nI(d^3)  1     16.9        95      148.9 3.869e-05 ***\nI(d^4)  1      7.9        94      140.9  0.004884 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m_d3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = cbind(y, n - y) ~ log(d) + d + I(d^2) + I(d^3) + \n    I(d^4), family = \"binomial\", data = simulated_putts)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  7.621e+00  2.642e-01  28.840  < 2e-16 ***\nlog(d)      -3.855e+00  1.899e-01 -20.304  < 2e-16 ***\nd            1.344e-01  2.069e-02   6.497 8.22e-11 ***\nI(d^2)      -1.313e-03  3.068e-04  -4.279 1.88e-05 ***\nI(d^3)       6.912e-06  2.106e-06   3.282  0.00103 ** \nI(d^4)      -1.420e-08  5.099e-09  -2.785  0.00536 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 24784.07  on 99  degrees of freedom\nResidual deviance:   140.93  on 94  degrees of freedom\nAIC: 611.94\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simulated_putts, \n       aes(x = d, y = y / n)) +\n  geom_point() +\n  geom_line(aes(y = predict(m_d3, type = \"response\"))) +\n  geom_line(aes(y = predict(m_d,  type = \"response\")), col= 'red') +\n  geom_line(aes(y = predict(m_d2,  type = \"response\")), col= 'blue')\n```\n\n::: {.cell-output-display}\n![](05-logistic-regression_files/figure-html/model-based-probabilities-1.png){width=672}\n:::\n:::",
    "supporting": [
      "05-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}