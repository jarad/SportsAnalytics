{
  "hash": "b6b69c08f63f7b3f36277a22df54a5ac",
  "result": {
    "engine": "knitr",
    "markdown": "# Regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\"); theme_set(theme_bw())\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4.9000     ✔ readr     2.1.5     \n✔ forcats   1.0.0          ✔ stringr   1.5.1     \n✔ ggplot2   3.5.1          ✔ tibble    3.2.1     \n✔ lubridate 1.9.3          ✔ tidyr     1.3.1     \n✔ purrr     1.0.2          \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(\"ggResidpanel\")\n```\n:::\n\n\n\n\n## Simple Linear Regression\n\n### Model\n\nFor observation $i = \\{1,2,\\ldots,n\\}$, let\n\n- $Y_i$ be the response variable and\n- $X_i$ be the explanatory variable.\n\nThe simple linear regression model (SLR) assumes\n$$ \nY_i \\stackrel{ind}{\\sim} N(\\beta_0 + \\beta_1 X_i, \\sigma^2)\n$$\nor, equivalently,\n$$ \nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0, \\sigma^2).\n$$\n\n### Interpretation\n\nRecall\n$$ \nE[Y_i] = \\beta_0 + \\beta_1 X_i\n$$\n\nThus,\n\n- $\\beta_0$ is the expected response when $X_i=0$ \n- $\\beta_1$ is the expected increase in the response when $X_i$ is increased by 1.\n\n\n\n\n### Assumptions\n\nRecall\n$$ \nE[Y_i] = \\beta_0 + \\beta_1 X_i, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0, \\sigma^2)\n$$\n\nThus, the model assumptions are \n\n- The errors are independent.\n- The errors are normally distributed.\n- The errors have constant variance.\n- The relationship between the expected response and the explanatory variable\nis a straight line. \n\n\n\n### Diagnostics\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(Sepal.Length ~ Sepal.Width, data = iris)\nggResidpanel::resid_panel(m, \n                          plots = c(\"resid\", \"qq\", \"cookd\"), \n                          qqbands = TRUE, \n                          nrow = 1)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/slr-diagnostics-1.png){width=672}\n:::\n:::\n\n\n\n\n### Triathlon Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- read_csv(\"data/ironman_lake_placid_female_2022_canadian.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 64 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): Name, Country, Gender, Division, Finish.Status, Location\ndbl (11): Bib, Division.Rank, Overall.Time, Overall.Rank, Swim.Time, Swim.Ra...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n```{.r .cell-code}\nhead(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 17\n    Bib Name     Country Gender Division Division.Rank Overall.Time Overall.Rank\n  <dbl> <chr>    <chr>   <chr>  <chr>            <dbl>        <dbl>        <dbl>\n1     2 Melanie… Canada  Female FPRO                 5         575.           21\n2     9 Pamela-… Canada  Female FPRO                10         610.           51\n3  1000 Carley … Canada  Female F35-39               4         660.          126\n4  1935 Seanna … Canada  Female F45-49               3         665.          131\n5   511 Marie-C… Canada  Female F45-49               4         679.          161\n6  1240 Julie H… Canada  Female F40-44               6         693.          202\n# ℹ 9 more variables: Swim.Time <dbl>, Swim.Rank <dbl>, Bike.Time <dbl>,\n#   Bike.Rank <dbl>, Run.Time <dbl>, Run.Rank <dbl>, Finish.Status <chr>,\n#   Location <chr>, Year <dbl>\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d |> filter(Swim.Time < 500), \n       aes(x = Swim.Time, y = Bike.Time)) + \n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/triathlon-data-plot-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(Bike.Time ~ Swim.Time, data = d |> filter(Swim.Time < 500))\nggResidpanel::resid_panel(m, plots = c(\"resid\", \"qq\", \"cookd\"), qqbands = TRUE, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/triathlon-model-diagnostics-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Bike.Time ~ Swim.Time, data = filter(d, Swim.Time < \n    500))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-68.901 -23.468  -2.169  23.808  70.369 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 188.8604    32.0893   5.885 1.82e-07 ***\nSwim.Time     2.8729     0.4035   7.120 1.44e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 34.83 on 61 degrees of freedom\nMultiple R-squared:  0.4538,\tAdjusted R-squared:  0.4449 \nF-statistic: 50.69 on 1 and 61 DF,  p-value: 1.443e-09\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m), confint(m))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            2.5 %     97.5 %\n(Intercept) 188.860386 124.693942 253.026829\nSwim.Time     2.872855   2.065987   3.679724\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4538433\n```\n\n\n:::\n:::\n\n\n\n\nWhen swim time is 0, the expected Bike Time is \n189 mins with a 95\\% interval of \n(125, 253).\nFor additional minute of swim time, \nthe bike time is expected to increase \n2.9 mins  \n(2.1, 3.7).\nThe model explains \n45\\% of the variability\nin bike time.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d |> filter(Swim.Time < 500), \n       aes(x = Swim.Time, y = Bike.Time)) + \n  geom_point() + geom_smooth(method = \"lm\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/triathlon-slr-plot-1.png){width=672}\n:::\n:::\n\n\n\n\n### Two-sample T-test\n\nWe can use SLR to compare two groups. \n\nNote that \n$$ \nY_i \\stackrel{ind}{\\sim} N(\\mu_{g[i]}, \\sigma^2) \n$$\nwhere $g[i] \\in \\{1,2\\}$ determines the group membership for observation $i$\n\nis equivalent to \n$$ \nY_i \\stackrel{ind}{\\sim} N(\\beta_0 + \\beta_1 \\mathrm{I}(g[i] = 2), \\sigma^2) \n$$\nwhere $\\mathrm{I}(g[i] = 2)$ is the indicator function,  i.e.\n$$\nI(A) = \\left\\{ \n\\begin{array}{ll}\n1 & A\\mbox{ is TRUE} \\\\\n0 & \\mbox{otherwise}\n\\end{array}\n\\right.\n$$\n%  i.e. $I(A) = 1$ if $A$ is true and $I(A) = 0$ otherwise,\n\nand\n$$\n\\mu_1 = \\beta_0 \\quad \\mbox{and} \\quad \\mu_2 = \\beta_0 + \\beta_1.\n$$\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd2 <- d |> filter(Division %in% c(\"F40-44\", \"F45-49\"))\n\nd2 |>\n  group_by(Division) |>\n  summarize(\n    n = n(),\n    mean = mean(Bike.Time),\n    sd = sd(Bike.Time)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  Division     n  mean    sd\n  <chr>    <int> <dbl> <dbl>\n1 F40-44      13  435.  43.6\n2 F45-49      18  405.  39.5\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d2, aes(x = Division, y = Bike.Time)) + \n  geom_boxplot(outliers = FALSE, color = \"gray\") + geom_jitter(width = 0.1)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/triathlon-subgroup-boxplot-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(Bike.Time ~ Division, data = d2)\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Bike.Time ~ Division, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-61.663 -32.237   3.556  27.806  95.406 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      435.13      11.44  38.040   <2e-16 ***\nDivisionF45-49   -29.97      15.01  -1.996   0.0554 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.24 on 29 degrees of freedom\nMultiple R-squared:  0.1208,\tAdjusted R-squared:  0.09051 \nF-statistic: 3.986 on 1 and 29 DF,  p-value: 0.05535\n```\n\n\n:::\n:::\n\n\n\n\nComparison to two-sample t-test:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m), confint(m)) # Regression results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            2.5 %      97.5 %\n(Intercept)    435.1295 411.73435 458.5246236\nDivisionF45-49 -29.9693 -60.67155   0.7329461\n```\n\n\n:::\n\n```{.r .cell-code}\nt.test(Bike.Time ~ Division, \n       data = d2, \n       var.equal = TRUE) # Two-sample t-test\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  Bike.Time by Division\nt = 1.9964, df = 29, p-value = 0.05535\nalternative hypothesis: true difference in means between group F40-44 and group F45-49 is not equal to 0\n95 percent confidence interval:\n -0.7329461 60.6715501\nsample estimates:\nmean in group F40-44 mean in group F45-49 \n            435.1295             405.1602 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n## Multiple Linear Regression\n\n### Model\n\nFor observation $i = \\{1,2,\\ldots,n \\}$, let\n\n- $Y_i$ be the value of the response variable and\n- $X_{i,j}$ be value of the $j$th explanatory variable \n\n\nThe (multiple linear) regression model  assumes\n$$ \nY_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_p X_{i,p} + \\epsilon_i\n$$\n\nand \n$$\n\\epsilon_i \\stackrel{ind}{\\sim} N(0, \\sigma^2).\n$$\n\n\n### Interpretation\n\nRecall\n$$\nE[Y_i] = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_p X_{i,p}\n$$\n\n\nThus,\n\n- $\\beta_0$ is the expected response when all $X_{i,j} = 0$ \n- $\\beta_j$ is the expected increase in the response when $X_{i,j}$ is increased by 1 and all other explanatory variables are held constant\n\n\nWhen multiple regression is used, \nyou will often see people write the phrases\n``after controlling for'' or ``after adjusting for'' followed by a list of \nthe other explanatory variables in the model.\n\n\n### Assumptions\n\n\nRecall\n$$\nE[Y_i] = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_p X_{i,p}, \\quad \\epsilon_i \\stackrel{ind}{\\sim} N(0, \\sigma^2)\n$$\n\n\nThus, the model assumptions are \n\n- The errors are independent.\n- The errors are normally distributed.\n- The errors have constant variance. \n- The relationship between the expected response and the explanatory variables is given above.\n\n\n### Diagnostics\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(Run.Time ~ Swim.Time + Bike.Time, data = d |> filter(Swim.Time < 500))\nggResidpanel::resid_panel(m, plots = c(\"resid\", \"qq\", \"cookd\"), \n                          qqbands = TRUE, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/multiple-regression-diagnostics-panel-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggResidpanel::resid_xpanel(m)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/multiple-regression-diagnostics-xpanel-1.png){width=672}\n:::\n:::\n\n\n\n\n### Example\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d |> filter(Swim.Time < 500),\n       aes(x = Swim.Time, y = Run.Time, color = Bike.Time)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/multiple-regression-plot-run-v-swim-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d |> filter(Swim.Time < 500),\n       aes(x = Bike.Time, y = Run.Time, color = Swim.Time)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/multiple-regression-plot-run-v-bike-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Run.Time ~ Swim.Time + Bike.Time, data = filter(d, \n    Swim.Time < 500))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-57.474 -16.782  -3.523  21.298  63.349 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -68.0058    35.1258  -1.936   0.0576 .  \nSwim.Time    -0.3771     0.4773  -0.790   0.4326    \nBike.Time     0.9726     0.1119   8.689  3.3e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 30.45 on 60 degrees of freedom\nMultiple R-squared:  0.6711,\tAdjusted R-squared:  0.6602 \nF-statistic: 61.22 on 2 and 60 DF,  p-value: 3.238e-15\n```\n\n\n:::\n:::\n\n\n\n\nWritten summary\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m), confint(m))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                              2.5 %   97.5 %\n(Intercept) -68.0057881 -138.267938 2.256362\nSwim.Time    -0.3771479   -1.331933 0.577637\nBike.Time     0.9725912    0.748696 1.196486\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6711405\n```\n\n\n:::\n:::\n\n\n\n\n\nUsing the 2022 Women's Lake Placid Ironman data, \nwe fit a regression model using run time as the response variable and \nswim and bike times as the explanatory variables. \nAfter adjusting for bike time, each minute increase of swim time was \nassociated with a -0.38 minute increase in run time with a \n95\\% interval of \n(-1.33, 0.58).\nAfter adjusting for swim time, each minute increase of bike time was \nassociated with a 0.97\n(0.75, 1.2)\nminute increase in run time.\nThe model with swim and bike time accounted for \n67\\% of the variability in run time.\n\n## ANOVA\n\nWhen our explanatory variable is categorical with more than 2 levels, \nwe can fit a regression model that will often be referred to as an ANOVA model. \n\nTo fit this model, we do the following\n\n- Choose one level to be the reference level \n(by default R will choose the level that comes first alphabetically) \n- Create indicator variables for all the other levels, i.e. \n$$ \n\\mathrm{I}(\\mbox{level for observation $i$ is $<$level$>$}) = \\left\\{\n\\begin{array}{ll}\n1 & \\mbox{if level for observation $i$ is $<$level$>$} \\\\\n0 & \\mbox{otherwise}\n\\end{array} \\right. \n$$ \n- Fit a regression model using these indicators.\n\nMost statistical software will perform these actions for you, \nbut it is useful to know this is what is happening.\n\nSummary statistics \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd |> group_by(Division) |> \n  summarize(\n    n    = n(),\n    mean = mean(Run.Time),\n    sd   = sd(Run.Time)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 9 × 4\n  Division     n  mean    sd\n  <chr>    <int> <dbl> <dbl>\n1 F25-29       6  317. 53.3 \n2 F30-34       4  308. 51.5 \n3 F35-39       8  283. 43.6 \n4 F40-44      13  327. 51.2 \n5 F45-49      18  301. 53.9 \n6 F50-54      11  311. 35.1 \n7 F55-59       1  400. NA   \n8 F65-69       1  264. NA   \n9 FPRO         2  210.  6.00\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d |> filter(Division != \"FPRO\"),\n       aes(x = Division, y = Run.Time)) +\n  geom_boxplot(outliers = FALSE, color = \"gray\") +\n  geom_jitter(width = 0.1)\n```\n\n::: {.cell-output-display}\n![](03-regression_files/figure-html/anova-plot-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nm <- lm(Run.Time ~ Division, data = d |> filter(Division != \"FPRO\"))\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Run.Time ~ Division, data = filter(d, Division != \n    \"FPRO\"))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-100.808  -35.221   -2.173   26.991  115.983 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     317.164     19.924  15.918   <2e-16 ***\nDivisionF30-34   -9.351     31.503  -0.297    0.768    \nDivisionF35-39  -33.816     26.358  -1.283    0.205    \nDivisionF40-44   10.260     24.087   0.426    0.672    \nDivisionF45-49  -15.747     23.007  -0.684    0.497    \nDivisionF50-54   -6.017     24.769  -0.243    0.809    \nDivisionF55-59   82.619     52.715   1.567    0.123    \nDivisionF65-69  -53.564     52.715  -1.016    0.314    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 48.8 on 54 degrees of freedom\nMultiple R-squared:  0.143,\tAdjusted R-squared:  0.03195 \nF-statistic: 1.288 on 7 and 54 DF,  p-value: 0.274\n```\n\n\n:::\n:::\n\n\n\n\n### F-test\n\nWhen evaluating the statistical support for including a categorical variable\nwith more than 2 levels, we use an F-test. \n\nThe hypotheses in an F-test are\n\n- $H_0: \\mu_g = \\mu$ (the means in all the groups are the same)\n- $H_1: \\mu_g \\ne \\mu_{g'}$ for some $g,g'$ \n(at least one mean is different)\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: Run.Time\n          Df Sum Sq Mean Sq F value Pr(>F)\nDivision   7  21469  3067.1  1.2877  0.274\nResiduals 54 128622  2381.9               \n```\n\n\n:::\n\n```{.r .cell-code}\ndrop1(m, test = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSingle term deletions\n\nModel:\nRun.Time ~ Division\n         Df Sum of Sq    RSS    AIC F value Pr(>F)\n<none>                128622 489.52               \nDivision  7     21469 150091 485.10  1.2877  0.274\n```\n\n\n:::\n\n```{.r .cell-code}\n# Alternatively fit two models and compare\nm0 <- lm(Run.Time ~ 1, data = d |> filter(Division != \"FPRO\"))\nanova(m0, m)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nModel 1: Run.Time ~ 1\nModel 2: Run.Time ~ Division\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1     61 150091                           \n2     54 128622  7     21469 1.2877  0.274\n```\n\n\n:::\n:::\n\n\n\n\nInterpretation\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(coef(m)[c(1, 3)], confint(m)[c(1, 3), ]) # divide by 60 to get hours\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                            2.5 %    97.5 %\n(Intercept)    317.16389 277.2179 357.10992\nDivisionF35-39 -33.81597 -86.6596  19.02766\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(m)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1430418\n```\n\n\n:::\n\n```{.r .cell-code}\nanova(m)$`Pr(>F)`[1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2740308\n```\n\n\n:::\n:::\n\n\n\n\nUsing the 2022 Women's Lake Placid Ironman data, \nwe fit a regression model using run time as the response variable and \nage division as the explanatory variable. \nThe mean run time for the F25-29 division was \n5.3 hours with a 95\\% interval of \n(4.6, 6).\nThere is evidence of a difference in mean run time amongst the divisions\n(ANOVA F-test p=`0.27`).\nThe estimated difference in run time for the F25-29 division minus the F35-39 \ndivision was\n34\n(-19, 87) minutes.\nThe model with division accounted for \n14\\% of the variability in run time.\n\n## Summary\n\n\n",
    "supporting": [
      "03-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}